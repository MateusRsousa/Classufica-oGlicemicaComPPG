# -*- coding: utf-8 -*-
"""PPG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTbZMZ_JEcBRpA9YUnbLY5ROSNIbA-UI
"""

# Instalar pacotes necessários
!pip install peakutils
!pip install heartpy
!pip install Orange3

import os
import pandas as pd
import numpy as np
import heartpy as hp
from scipy.signal import butter, filtfilt, savgol_filter
from scipy.stats import kurtosis, skew
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, balanced_accuracy_score, confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import balanced_accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt


import Orange
from Orange.data import Domain, ContinuousVariable, DiscreteVariable
from Orange.classification.rules import CN2Learner
from Orange.evaluation import CrossValidation, scoring, CA

from collections import Counter

from Orange.data.pandas_compat import table_from_frame

"""**CARREGAR DADOS**"""

# Montar o Google Drive
drive.mount('/content/drive')

# Função para carregar os dados de PPG dos pacientes
def load_ppg_data(folder_path):
    ppg_data = {}
    for patient_folder in sorted(os.listdir(folder_path)):
        patient_path = os.path.join(folder_path, patient_folder)
        if os.path.isdir(patient_path):
            for file in os.listdir(patient_path):
                if file.endswith('_wave.csv'):  # Processar apenas arquivos que terminam com '_wave.csv'
                    file_path = os.path.join(patient_path, file)
                    ppg_wave = pd.read_csv(file_path)['Wave']
                    ppg_data[patient_folder] = ppg_wave
    return ppg_data




# Carregar os dados de glicose (alta ou baixa) a partir do arquivo Excel
glicose_data_path = '/content/drive/My Drive/PPG_IEB/dados_estruturados.xlsx'
glicose_data = pd.read_excel(glicose_data_path)

# Criar um dicionário para mapear o ID do paciente (coluna 'Participante') à coluna 'Grupo'
glicose_dict = glicose_data.set_index('Participante')['Grupo'].apply(lambda x: 1 if x == 'D' else 0).to_dict()

# Ajustar os IDs de PPG para ter o mesmo formato dos IDs de glicose (adicionando o prefixo '#')
def format_ppg_id(patient_id):
    return f"#{patient_id.zfill(3)}"  # Garante que o ID tenha 3 dígitos e o prefixo '#'

# Caminho para a pasta com os dados PPG
ppg_folder_path = '/content/drive/My Drive/PPG_IEB/'

# Carregar os dados de PPG
ppg_data = load_ppg_data(ppg_folder_path)

"""**FILTROS**"""

def savitzky_golay_filter(data, window_length=11, polyorder=3):
    # Garantir que window_length seja ímpar e menor que o tamanho do sinal
    if window_length % 2 == 0:
        window_length += 1
    if window_length > len(data):
        window_length = len(data) - (len(data) % 2) - 1  # Ajustar para ser ímpar e menor que o tamanho do sinal
    # Aplicar o filtro
    filtered_data = savgol_filter(data, window_length=window_length, polyorder=polyorder)
    return filtered_data  # Retorna numpy.ndarray

# Função para aplicar o filtro Butterworth Bandpass
def butterworth_bandpass_filter(data, lowcut, highcut, fs, order=4):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    filtered_data = filtfilt(b, a, data)
    return filtered_data  # Retorna numpy.ndarray

# Função para aplicar a normalização Min-Max
def min_max_normalization(data):
    data_min = np.min(data)
    data_max = np.max(data)
    if data_max - data_min == 0:
        normalized_data = data - data_min  # Retorna zeros
    else:
        normalized_data = (data - data_min) / (data_max - data_min)
    return normalized_data  # Retorna numpy.ndarray

# Função para extrair características dos sinais PPG (média e variância)

# Função de cálculo dos parâmetros de Hjorth
def hjorth_parameters(signal):
    """Calcula mobilidade e complexidade de Hjorth."""
    first_derivative = np.diff(signal)  # Primeira derivada do sinal
    mobility = np.std(first_derivative) / np.std(signal)  # Mobilidade
    second_derivative = np.diff(first_derivative)  # Segunda derivada
    complexity = np.std(second_derivative) / np.std(first_derivative)  # Complexidade
    return mobility, complexity

# Função para calcular a entropia de amostra
def sample_entropy(signal):
    """Calcula a entropia de amostra."""
    n = len(signal)
    r = 0.2 * np.std(signal)
    A = sum([np.abs(signal[i] - signal[i+1]) < r for i in range(n-1)])
    B = sum([np.abs(signal[i] - signal[i+2]) < r for i in range(n-2)])
    return -np.log(A/B) if B != 0 else 0

# Função para calcular a energia do sinal
def signal_energy(signal):
    """Calcula a energia do sinal."""
    return np.sum(np.square(signal))

# Função para calcular a frequência cardíaca média
def heart_rate(peaks, sampling_rate):
    rr_intervals = np.diff(peaks) / sampling_rate  # Intervalos entre batimentos em segundos
    hr = 60.0 / np.mean(rr_intervals)  # Convertendo para batimentos por minuto
    return hr

# Função para calcular o tempo médio entre picos (RR intervals)
def mean_rr_interval(peaks, sampling_rate):
    rr_intervals = np.diff(peaks) / sampling_rate  # Intervalos entre picos em segundos
    return np.mean(rr_intervals)

# Função para calcular o tempo de subida e descida do sinal
def rise_fall_time(segment):
    diff_signal = np.diff(segment)
    rise_time = np.sum(diff_signal > 0)
    fall_time = np.sum(diff_signal < 0)
    return rise_time, fall_time

"""**COMPARANDO FILTROS**"""

# Selecionar um paciente e carregar seu sinal PPG
patient_id = list(ppg_data.keys())[0]  # Seleciona o primeiro paciente da lista
ppg_wave = ppg_data[patient_id]

# Certificar-se de que ppg_wave é um numpy.ndarray
if isinstance(ppg_wave, pd.Series):
    ppg_wave = ppg_wave.values

# Definir a taxa de amostragem
sampling_rate = 100  # Hz

# Aplicar o filtro Butterworth Bandpass ao sinal completo
filtered_ppg_butterworth = butterworth_bandpass_filter(ppg_wave, lowcut=0.5, highcut=15, fs=sampling_rate, order=6)

# Aplicar o filtro Savitzky-Golay ao sinal completo
filtered_ppg_savgol = savitzky_golay_filter(ppg_wave)

# Processar o sinal filtrado para obter picos e mínimos
wd_butter, _ = hp.process(filtered_ppg_butterworth, sampling_rate)
peaks_butter = np.array(wd_butter['peaklist'])
wd_inv_butter, _ = hp.process(-filtered_ppg_butterworth, sampling_rate)
minima_butter = np.array(wd_inv_butter['peaklist'])

# Selecionar um segmento do sinal filtrado pelo Butterworth
segments_butter = []
segment_indices = []

for i in range(0, len(peaks_butter) - 9, 10):
    start_min_candidates = minima_butter[minima_butter < peaks_butter[i]]
    end_min_candidates = minima_butter[minima_butter > peaks_butter[i + 9]]

    if len(start_min_candidates) > 0 and len(end_min_candidates) > 0:
        start_min = start_min_candidates[-1]
        end_min = end_min_candidates[0]
        segment_butter = filtered_ppg_butterworth[start_min:end_min]
        segments_butter.append(segment_butter)
        segment_indices.append((start_min, end_min))

# Verificar se temos segmentos disponíveis
if len(segments_butter) == 0:
    print("Não foram encontrados segmentos suficientes para o paciente selecionado.")
else:
    # Selecionar o primeiro segmento
    segment_butter = segments_butter[0]
    start_min, end_min = segment_indices[0]

    # Extrair o segmento correspondente do sinal original
    segment_original = ppg_wave[start_min:end_min]

    # Aplicar os filtros ao segmento do sinal original
    segment_savgol = savitzky_golay_filter(segment_original)
    segment_butter_segment = butterworth_bandpass_filter(segment_original, lowcut=0.5, highcut=15, fs=sampling_rate, order=4)

    # Plotar os sinais
    plt.figure(figsize=(12, 6))
    plt.plot(segment_original, label='Sinal Original (Sem Filtro)', color='gray')
    plt.plot(segment_savgol, label='Filtro Savitzky-Golay', color='blue')
    plt.plot(segment_butter_segment, label='Filtro Butterworth Bandpass', color='red')
    plt.legend()
    plt.title('Comparação dos Filtros em um Segmento do Sinal PPG')
    plt.xlabel('Amostras')
    plt.ylabel('Amplitude')
    plt.show()

"""**CONFIGURAÇÃO DE FEATURES**"""

def comparar_len_ppg_final(len_ppg_data, len_final_data):
    """
    Função para comparar o número de pacientes em `ppg_data` com o número de segmentos processados em `final_data`.

    Parâmetros:
        len_ppg_data (int): Número de pacientes em `ppg_data`.
        len_final_data (int): Número de segmentos processados em `final_data`.
    """
    plt.figure(figsize=(10, 6))
    plt.bar(['Número de Pacientes (ppg_data)', 'Número de Segmentos (final_data)'],
            [len_ppg_data, len_final_data], color=['#4A90E2', '#D0021B'])
    plt.ylabel('Quantidade')
    plt.title('Comparação entre o Número de Pacientes e Segmentos Processados')
    plt.show()




# Função para processar um único sinal PPG e extrair as características
def process_ppg_signal(ppg_signal, sampling_rate=100, lowcut=1, highcut=10):
    filtered_ppg = butterworth_bandpass_filter(ppg_signal,lowcut,highcut, 100)

    # Processa o sinal para obter os picos e mínimos
    wd, _ = hp.process(filtered_ppg, sampling_rate)
    peaks = np.array(wd['peaklist'])
    wd, _ = hp.process(-filtered_ppg, sampling_rate)
    minima = np.array(wd['peaklist'])

    # Segmentação do sinal em grupos de 5 períodos
    segments = []
    for i in range(0, len(peaks) - 9, 5):
        start_min_candidates = minima[minima < peaks[i]]
        end_min_candidates = minima[minima > peaks[i + 9]]

        if len(start_min_candidates) > 0 and len(end_min_candidates) > 0:
            start_min = start_min_candidates[-1]
            end_min = end_min_candidates[0]
            segment = filtered_ppg[start_min:end_min]
            segments.append(segment)

    # Calcular frequência cardíaca e intervalo RR médio (usando todos os picos do sinal)
    heart_rate_avg = heart_rate(peaks, sampling_rate)
    rr_interval_avg = mean_rr_interval(peaks, sampling_rate)

    return segments, heart_rate_avg, rr_interval_avg

# Inicializa o DataFrame para armazenar os resultados
results = []

print(len(ppg_data))

# Processa cada sinal de PPG e extrai características
for patient_id, ppg_wave in ppg_data.items():
    formatted_id = format_ppg_id(patient_id)  # Ajustar o formato do ID do paciente
    print(f"Processando paciente {formatted_id}...")

    # Processar o sinal PPG para obter segmentos e frequências
    segments, heart_rate_avg, rr_interval_avg = process_ppg_signal(ppg_wave)

    # Adicionar a informação de glicose alta/baixa (1 para alta, 0 para baixa)
    glicose_alta = glicose_dict.get(formatted_id, None)  # None se o ID não for encontrado

    # Calcular características do sinal completo (fora do loop de segmentos)
    mediana_wave = ppg_wave.median()
    curtose_wave = ppg_wave.kurt()
    media_movel_wave = ppg_wave.rolling(window=5).mean().mean()
    assimetria_wave = ppg_wave.skew()
    desvio_padrao_wave = ppg_wave.std()
    media_wave = ppg_wave.mean()
    var_wave = ppg_wave.var()
    min_wave = ppg_wave.min()
    max_wave = ppg_wave.max()

    # Para cada segmento extraído, armazena as características no DataFrame
    for segment in segments:
        # Converter segmento para pandas Series se não for
        if not isinstance(segment, pd.Series):
            segment = pd.Series(segment)

        # Calcular características do segmento
        avg = segment.mean()
        std_dev = segment.std()
        kurt = kurtosis(segment)
        skewness = skew(segment)
        entropy = sample_entropy(segment)
        energy = signal_energy(segment)
        mobility, complexity = hjorth_parameters(segment)
        rise_time, fall_time = rise_fall_time(segment)

        # Calcular média da média móvel no segmento
        media_movel_segment = segment.rolling(window=5).mean().mean()

        result = {
            'Patient_ID': formatted_id,
            'Std_Dev': std_dev,
            'Kurtosis': kurt,
            'Skew': skewness,
            'Entropy': entropy,
            'Energy': energy,
            'Hjorth_Mobility': mobility,
            'Hjorth_Complexity': complexity,
            'Heart_Rate_Avg': heart_rate_avg,
            'RR_Interval_Avg': rr_interval_avg,
            'Rise_Time': rise_time,
            'Fall_Time': fall_time,
            'Glicose_Alta': glicose_alta,  # Adicionar a coluna de Glicose Alta/Baixa
            'mediana_wave': mediana_wave,  # Mediana do sinal completo
            'curtose_wave': curtose_wave,  # Curtose do sinal completo
            'media_movel_wave': media_movel_wave,  # Média da média móvel do sinal completo
            'assimetria_wave': assimetria_wave,  # Assimetria do sinal completo
            'desvio_padrao_wave': desvio_padrao_wave,  # Desvio padrão do sinal completo
            'media_wave': media_wave,  # Média do sinal completo
            'var_wave': var_wave,  # Variância do sinal completo
            'min_wave': min_wave,  # Valor mínimo do sinal completo
            'max_wave': max_wave   # Valor máximo do sinal completo
        }
        results.append(result)

# Convertendo os resultados em um DataFrame
final_data = pd.DataFrame(results)

# Exibindo o DataFrame com as características extraídas
print(final_data)
print(len(final_data))

# Salvando o DataFrame em um arquivo CSV
comparar_len_ppg_final(len(ppg_data), len(final_data))

final_data.to_csv('final_data.csv', index=False)

from google.colab import files
files.download('final_data.csv')

"""**TREINANDO O MODELO RANDOM FOREST**"""

# Função para criar o gráfico de barras comparando cada modelo ao 100%
def plot_model_accuracy_vs_100(model_accuracies):
    models = list(model_accuracies.keys())
    accuracies = [acc * 100 for acc in model_accuracies.values()]  # Converte para porcentagem

    plt.figure(figsize=(10, 6))
    bars = plt.bar(models, accuracies, color=['skyblue', 'salmon', 'lightgreen', 'orange'])
    plt.axhline(100, color='red', linestyle='--', label='100% de Acurácia')
    plt.xlabel("Modelos")
    plt.ylabel("Acurácia (%)")
    plt.title("Comparação de Acurácia dos Modelos com 100%")
    plt.ylim(0, 110)
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()

    # Exibe valores no topo de cada barra para melhor visualização
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f'{yval:.2f}%', ha='center', va='bottom')

    plt.show()


# Dividir dados em treino e teste
X = final_data.drop(['Glicose_Alta', 'Patient_ID'], axis=1)
y = final_data['Glicose_Alta']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Treinar modelo Naive Bayes
model_nb = GaussianNB()
model_nb.fit(X_train, y_train)

accuracy_nb = model_nb.score(X_test, y_test)
print(f'Acurácia do modelo Naive Bayes: {accuracy_nb * 100:.2f}%')

# Treinar modelo Logistic Regression
clf_logit = LogisticRegression(max_iter=1000000000000000)
clf_logit.fit(X_train, y_train)

y_pred_logit = clf_logit.predict(X_test)

balanced_accuracy_logit = balanced_accuracy_score(y_test, y_pred_logit)
print(f'Balanced Accuracy Logistic Regression: {balanced_accuracy_logit * 100:.2f}%')

# Treinar modelo k-NN
clf_knn = KNeighborsClassifier(n_neighbors=100)
clf_knn.fit(X_train, y_train)

y_pred_knn = clf_knn.predict(X_test)

acc_knn = balanced_accuracy_score(y_test, y_pred_knn)
print(f"Acurácia balanceada para k-NN: {acc_knn * 100:.2f}%")

# Treinar modelo Random Forest
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clf_rf.fit(X_train, y_train)

y_pred_rf = clf_rf.predict(X_test)
acc_rf = balanced_accuracy_score(y_test, y_pred_rf)
print(f"Balanced Accuracy Random Forest: {acc_rf * 100:.2f}%")

# Dicionário com as acurácias balanceadas dos modelos
model_accuracies = {
    'Naive Bayes': accuracy_nb,
    'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    'k-Nearest Neighbors (k=100)': acc_knn,
    'Random Forest (estimator|states=100|42)': acc_rf
}


plot_model_accuracy_vs_100(model_accuracies)

"""**ESTATISTICAS DO MODELO RANDOM FOREST**"""

# Plotar a Matriz de Confusão
cm = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Baixa', 'Alta'], yticklabels=['Baixa', 'Alta'])
plt.ylabel('Verdadeiro')
plt.xlabel('Previsto')
plt.title('Matriz de Confusão - Random Forest')
plt.show()

# Exibir Relatório de Classificação
print("Relatório de Classificação - Random Forest:")
print(classification_report(y_test, y_pred_rf, target_names=['Baixa', 'Alta']))



# Plotar a Importância das Características
importances = clf_rf.feature_importances_
feature_names = X.columns

feature_importance_df = pd.DataFrame({'Características': feature_names, 'Importância': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importância', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importância', y='Características', data=feature_importance_df)
plt.title('Importância das Características - Random Forest')
plt.xlabel('Importância')
plt.ylabel('Características')
plt.tight_layout()
plt.show()

# Previsões no conjunto de teste para Naive Bayes
y_pred_nb = model_nb.predict(X_test)

accuracy_nb = model_nb.score(X_test, y_test)
print(f'Acurácia do modelo Naive Bayes: {accuracy_nb * 100:.2f}%')

# Calcular a matriz de confusão
cm_nb = confusion_matrix(y_test, y_pred_nb)

# Plotar a matriz de confusão
plt.figure(figsize=(6, 4))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', xticklabels=['Baixa', 'Alta'], yticklabels=['Baixa', 'Alta'])
plt.ylabel('Valor Real')
plt.xlabel('Previsão')
plt.title('Matriz de Confusão - Naive Bayes')
plt.show()

# Exibir relatório de classificação
print("Relatório de Classificação - Naive Bayes:")
print(classification_report(y_test, y_pred_nb, target_names=['Baixa', 'Alta']))

# Importância das Características - Naive Bayes
feature_names = X.columns
theta = model_nb.theta_  # Médias das características condicionadas a cada classe

# Criar um DataFrame com as médias
feature_importance_nb = pd.DataFrame(theta, columns=feature_names)
feature_importance_nb['Classe'] = ['Baixa', 'Alta']

# Transpor o DataFrame para facilitar a plotagem
feature_importance_nb = feature_importance_nb.set_index('Classe').T

# Plotar as médias das características por classe
feature_importance_nb.plot(kind='bar', figsize=(12, 6))
plt.title('Médias das Características por Classe - Naive Bayes')
plt.xlabel('Características')
plt.ylabel('Média')
plt.legend(title='Classe')
plt.tight_layout()
plt.show()

balanced_accuracy_logit = balanced_accuracy_score(y_test, y_pred_logit)
print(f'Balanced Accuracy Logistic Regression: {balanced_accuracy_logit * 100:.2f}%')

# Calcular a matriz de confusão
cm_logit = confusion_matrix(y_test, y_pred_logit)

# Plotar a matriz de confusão
plt.figure(figsize=(6, 4))
sns.heatmap(cm_logit, annot=True, fmt='d', cmap='Greens', xticklabels=['Baixa', 'Alta'], yticklabels=['Baixa', 'Alta'])
plt.ylabel('Valor Real')
plt.xlabel('Previsão')
plt.title('Matriz de Confusão - Regressão Logística')
plt.show()

# Exibir relatório de classificação
print("Relatório de Classificação - Regressão Logística:")
print(classification_report(y_test, y_pred_logit, target_names=['Baixa', 'Alta']))

# Importância das Características - Regressão Logística
coefficients = clf_logit.coef_[0]
feature_names = X.columns

# Criar um DataFrame com os coeficientes
feature_importance_logit = pd.DataFrame({'Características': feature_names, 'Coeficiente': coefficients})

# Calcular o valor absoluto dos coeficientes
feature_importance_logit['Coeficiente_Abs'] = feature_importance_logit['Coeficiente'].abs()

# Ordenar por importância
feature_importance_logit = feature_importance_logit.sort_values(by='Coeficiente_Abs', ascending=False)

# Plotar as importâncias das características
plt.figure(figsize=(10, 6))
sns.barplot(x='Coeficiente', y='Características', data=feature_importance_logit)
plt.title('Importância das Características - Regressão Logística')
plt.xlabel('Coeficiente')
plt.ylabel('Características')
plt.tight_layout()
plt.show()

acc_knn = balanced_accuracy_score(y_test, y_pred_knn)
print(f"Acurácia balanceada para k-NN: {acc_knn * 100:.2f}%")

# Calcular a matriz de confusão
cm_knn = confusion_matrix(y_test, y_pred_knn)

# Plotar a matriz de confusão
plt.figure(figsize=(6, 4))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Oranges', xticklabels=['Baixa', 'Alta'], yticklabels=['Baixa', 'Alta'])
plt.ylabel('Valor Real')
plt.xlabel('Previsão')
plt.title('Matriz de Confusão - k-NN')
plt.show()

# Exibir relatório de classificação
print("Relatório de Classificação - k-NN:")
print(classification_report(y_test, y_pred_knn, target_names=['Baixa', 'Alta']))

# Importância das Características - k-NN (Permutation Importance)
from sklearn.inspection import permutation_importance

# Calcular a importância por permutação
result = permutation_importance(clf_knn, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

# Criar um DataFrame com as importâncias
feature_importance_knn = pd.DataFrame({'Características': X.columns, 'Importância': result.importances_mean})
feature_importance_knn = feature_importance_knn.sort_values(by='Importância', ascending=False)

# Plotar as importâncias das características
plt.figure(figsize=(10, 6))
sns.barplot(x='Importância', y='Características', data=feature_importance_knn)
plt.title('Importância das Características - k-NN (Permutation Importance)')
plt.xlabel('Importância')
plt.ylabel('Características')
plt.tight_layout()
plt.show()

"""**CN2**"""

# Definir o domínio (features e target)
domain = Orange.data.Domain(
    [Orange.data.ContinuousVariable(name) for name in X_train.columns],
    Orange.data.DiscreteVariable('Glicose_Alta', values=['Baixa', 'Alta'])  # Ajuste os nomes conforme necessário
)

# Criar tabelas do Orange
train_data = Orange.data.Table(domain, X_train.values, y_train.values.reshape(-1, 1))
test_data = Orange.data.Table(domain, X_test.values, y_test.values.reshape(-1, 1))

# Treinar o modelo CN2
learner_cn2 = Orange.classification.rules.CN2Learner()
clf_cn2 = learner_cn2(train_data)

# Fazer previsões com o modelo CN2
y_pred_cn2 = [clf_cn2(x) for x in test_data]

# Calcular a acurácia balanceada
acc_cn2 = balanced_accuracy_score(y_test, y_pred_cn2)
print(f"Acurácia balanceada para CN2: {acc_cn2 * 100:.2f}%")

# Matriz de confusão do CN2
cm_cn2 = confusion_matrix(y_test, y_pred_cn2)

# Plotar a matriz de confusão
plt.figure(figsize=(6, 4))
sns.heatmap(cm_cn2, annot=True, fmt='d', cmap='Purples', xticklabels=['Baixa', 'Alta'], yticklabels=['Baixa', 'Alta'])
plt.ylabel('Valor Real')
plt.xlabel('Previsão')
plt.title('Matriz de Confusão - CN2')
plt.show()

for rule in clf_cn2.rule_list:
    print(rule)
# Exibir relatório de classificação
print("Relatório de Classificação - CN2:")
print(classification_report(y_test, y_pred_cn2, target_names=['Baixa', 'Alta']))

model_accuracies = {
    'Naive Bayes': accuracy_nb,
    'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    'k-Nearest Neighbors (k=100)': acc_knn,
    'Random Forest (estimator|states=100|42)': acc_rf,
    'CN2': acc_cn2
}

plot_model_accuracy_vs_100(model_accuracies)

"""**---------------------------------------OTIMIZANDO PARAMETROS---------------------------**"""

# --- Implementação da Otimização de Parâmetros ---

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Remover quaisquer linhas com valores nulos
final_data_clean = final_data.dropna()

# Separar features e target
X = final_data_clean.drop(['Patient_ID', 'Glicose_Alta'], axis=1)
y = final_data_clean['Glicose_Alta']

# Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Definir o modelo
rf = RandomForestClassifier(random_state=42)

# Definir o grid de hiperparâmetros para otimização
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

# Configurar o GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='accuracy')

# Treinar o modelo com a otimização de hiperparâmetros
grid_search.fit(X_train, y_train)

# Melhor modelo encontrado
best_rf = grid_search.best_estimator_

# Previsões no conjunto de teste
y_pred = best_rf.predict(X_test)

# Relatório de classificação
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Matriz de confusão
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Plotar a matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Glicose Normal', 'Glicose Alta'], yticklabels=['Glicose Normal', 'Glicose Alta'])
plt.title('Matriz de Confusão')
plt.xlabel('Predição')
plt.ylabel('Real')
plt.show()

# Imprimir os melhores parâmetros e a pontuação correspondente
print("Melhores Parâmetros:", grid_search.best_params_)
print("Melhor Pontuação:", grid_search.best_score_)

acc_rf_best = balanced_accuracy_score(y_test, y_pred)
print(f"Balanced Accuracy Random Forest (otimizado): {acc_rf * 100:.2f}%")

# --- Gráfico de Barras para os Melhores Parâmetros e Pontuação ---
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Criar dados para o gráfico
params = list(best_params.keys())
values = list(best_params.values())

# Adicionando a pontuação ao final
params.append('Melhor Pontuação')
values.append(best_score)

# Criar o gráfico de barras
plt.figure(figsize=(10, 6))
sns.barplot(x=params, y=values, palette='viridis')
plt.title('Melhores Parâmetros e Pontuação do Modelo Random Forest')
plt.xlabel('Parâmetros')
plt.ylabel('Valor')
plt.xticks(rotation=45)
plt.show()

model_accuracies_otimization = {
    #'Naive Bayes': accuracy_nb,
    #'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    #'k-Nearest Neighbors (k=100)': acc_knn,
    #'Random Forest (estimator|states=100|42)': acc_rf,
    #'CN2': acc_cn2
    'Random Forest': acc_rf_best
}

plot_model_accuracy_vs_100(model_accuracies_otimization)

"""**ALTERNATIVO**"""

# Treinar o modelo final com todos os dados de treinamento
best_rf.fit(X_train, y_train)

# Obter a importância das features
feature_importances = best_rf.feature_importances_

# Criar um DataFrame para visualizar as importâncias
importances_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
})

# Ordenar por importância
importances_df = importances_df.sort_values(by='Importance', ascending=False)

print(importances_df)

# Plotar as importâncias
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=importances_df)
plt.title('Importância das Features')
plt.show()


# Definir o limiar
threshold = 0.06

# Selecionar as features com importância maior que o limiar
selected_features = importances_df[importances_df['Importance'] > threshold]['Feature']

print("Features selecionadas:")
print(selected_features)

# Atualizar X_train e X_test com as features selecionadas
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Treinar o modelo com as features selecionadas
best_rf.fit(X_train_selected, y_train)

# Fazer previsões
y_pred_selected = best_rf.predict(X_test_selected)

# Avaliar o modelo
print("Classification Report after Feature Selection:")
print(classification_report(y_test, y_pred_selected))

print("Confusion Matrix after Feature Selection:")
print(confusion_matrix(y_test, y_pred_selected))

acc_rf_selecao_alt = balanced_accuracy_score(y_test, y_pred_selected)
print(f"Balanced Accuracy Random Forest (otimizado): {acc_rf_selecao_alt * 100:.2f}%")

# --- Plotar a Matriz de Confusão ---
conf_matrix = confusion_matrix(y_test, y_pred_selected)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Glicose Baixa', 'Glicose Alta'],
            yticklabels=['Glicose Baixa', 'Glicose Alta'])
plt.title('Matriz de Confusão')
plt.xlabel('Previsão')
plt.ylabel('Real')
plt.show()

# --- Importância das Features para o Modelo `best_rf` ---
# Obter a importância das features
feature_importances = best_rf.feature_importances_

# Criar um DataFrame para visualizar as importâncias
importances_dfalt = pd.DataFrame({
    'Feature': X_train_selected.columns,
    'Importance': feature_importances
})

# Ordenar por importância
importances_dfalt = importances_df.sort_values(by='Importance', ascending=False)

# Plotar as importâncias
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importances_dfalt)
plt.title('Importância das Features para o Modelo Random Forest Otimizado')
plt.xlabel('Importância')
plt.ylabel('Features')
plt.show()

# --- Gráfico de Importância das Features Selecionadas ---
# Obter a importância das features selecionadas
selected_importances = importances_dfalt[importances_dfalt['Feature'].isin(selected_features)]

# Plotar as importâncias das features selecionadas
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=selected_importances, palette='viridis')
plt.title('Importância das Features Selecionadas')
plt.xlabel('Importância')
plt.ylabel('Features')
plt.show()

model_accuracies_otimization_2 = {
    #'Naive Bayes': accuracy_nb,
    #'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    #'k-Nearest Neighbors (k=100)': acc_knn,
    #'Random Forest (estimator|states=100|42)': acc_rf,
    #'CN2': acc_cn2,
    #'Random Forest Grid': acc_rf_best,
    'Random Forest Seleção de features': acc_rf_selecao_alt
}

plot_model_accuracy_vs_100(model_accuracies_otimization_2)

"""**COMPARANDO RESULTADOS**

**RFE RANDOM FOREST**
"""

from sklearn.feature_selection import RFE

# Definir o modelo base
rf = RandomForestClassifier(random_state=42, n_estimators=100)

# Definir o RFE
n_features_to_select = 4  # Número de features que queremos selecionar
rfe = RFE(estimator=rf, n_features_to_select=n_features_to_select)

# Ajustar o RFE nos dados de treinamento
rfe.fit(X_train, y_train)

# Obter as features selecionadas
selected_features_rfe = X_train.columns[rfe.support_]

print("Features selecionadas pelo RFE:")
print(selected_features_rfe)

# Atualizar X_train e X_test com as features selecionadas
X_train_rfe = X_train[selected_features_rfe]
X_test_rfe = X_test[selected_features_rfe]

# Treinar o modelo com as features selecionadas
rf.fit(X_train_rfe, y_train)

# Fazer previsões
y_pred_rfe = rf.predict(X_test_rfe)

# Avaliar o modelo
print("Classification Report after RFE:")
print(classification_report(y_test, y_pred_rfe))

print("Confusion Matrix after RFE:")
print(confusion_matrix(y_test, y_pred_rfe))

acc_rf_rfe = balanced_accuracy_score(y_test, y_pred_rfe)
print(f"Balanced Accuracy Random Forest (otimizado): {acc_rf_rfe * 100:.2f}%")

# --- Plotar a Importância das Features selecionadas pelo RFE ---
# Obter a importância das features
# --- Plotar a Matriz de Confusão ---
conf_matrix = confusion_matrix(y_test, y_pred_rfe)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Glicose Baixa', 'Glicose Alta'],
            yticklabels=['Glicose Baixa', 'Glicose Alta'])
plt.title('Matriz de Confusão após Seleção de Features com RFE')
plt.xlabel('Previsão')
plt.ylabel('Real')
plt.show()

feature_importances = rf.feature_importances_

# Criar um DataFrame para visualizar as importâncias
importances_df_rfe = pd.DataFrame({
    'Feature': X_train_rfe.columns,
    'Importance': feature_importances
})

# Ordenar por importância
importances_df_rfe = importances_df_rfe.sort_values(by='Importance', ascending=False)

# Plotar as importâncias
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importances_df_rfe, palette='viridis')
plt.title('Importância das Features Selecionadas pelo RFE')
plt.xlabel('Importância')
plt.ylabel('Features')
plt.show()


model_accuracies_otimization_4 = {
    #'Naive Bayes': accuracy_nb,
    #'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    #'k-Nearest Neighbors (k=100)': acc_knn,
    #'Random Forest (estimator|states=100|42)': acc_rf,
    #'CN2': acc_cn2,
    #'Random Forest Grid': acc_rf_best,
    'Random Forest RFE': acc_rf_rfe
}

plot_model_accuracy_vs_100(model_accuracies_otimization_4)

"""**SEKECT K BEST - RANDOM FOREST**"""

from sklearn.feature_selection import SelectKBest, f_classif

# Definir o número de features que queremos selecionar
k = 9

# Aplicar o SelectKBest
selector = SelectKBest(score_func=f_classif, k=k)
selector.fit(X_train, y_train)

# Obter as features selecionadas
selected_features_kbest = X_train.columns[selector.get_support()]

print("Features selecionadas pelo SelectKBest:")
print(selected_features_kbest)

# Atualizar X_train e X_test com as features selecionadas
X_train_kbest = X_train[selected_features_kbest]
X_test_kbest = X_test[selected_features_kbest]

# Treinar o modelo com as features selecionadas
rf.fit(X_train_kbest, y_train)

# Fazer previsões
y_pred_kbest = rf.predict(X_test_kbest)

# Avaliar o modelo
print("Classification Report after SelectKBest:")
print(classification_report(y_test, y_pred_kbest))

print("Confusion Matrix after SelectKBest:")
print(confusion_matrix(y_test, y_pred_kbest))

acc_rf_k_best = balanced_accuracy_score(y_test, y_pred_kbest)
print(f"Balanced Accuracy Random Forest (otimizado): {acc_rf_k_best * 100:.2f}%")

# --- Plotar a Importância das Features selecionadas pelo SelectKBest ---
# Obter a importância das features para as selecionadas
feature_importances_kbest = rf.feature_importances_

# Criar um DataFrame para visualizar as importâncias
importances_df_kbest = pd.DataFrame({
    'Feature': X_train_kbest.columns,
    'Importance': feature_importances_kbest
})

# Ordenar por importância
importances_df_kbest = importances_df_kbest.sort_values(by='Importance', ascending=False)

# Plotar as importâncias
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importances_df_kbest, palette='viridis')
plt.title('Importância das Features Selecionadas pelo SelectKBest')
plt.xlabel('Importância')
plt.ylabel('Features')
plt.show()

# --- Plotar a Matriz de Confusão ---
conf_matrix_kbest = confusion_matrix(y_test, y_pred_kbest)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_kbest, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Glicose Baixa', 'Glicose Alta'],
            yticklabels=['Glicose Baixa', 'Glicose Alta'])
plt.title('Matriz de Confusão após Seleção de Features com SelectKBest')
plt.xlabel('Previsão')
plt.ylabel('Real')
plt.show()

model_accuracies_otimization_3 = {
    #'Naive Bayes': accuracy_nb,
    #'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    #'k-Nearest Neighbors (k=100)': acc_knn,
    #'Random Forest (estimator|states=100|42)': acc_rf,
    #'CN2': acc_cn2,
    #'Random Forest Grid': acc_rf_best,
    #'Random Forest Seleção de features': acc_rf_selecao,
    #'Random Forest RFE': acc_rf_rfe,
    'Random Forest SelectKBest': acc_rf_k_best
}

plot_model_accuracy_vs_100(model_accuracies_otimization_3)

"""**COMPARANDO TODOS OS MÉTODOS**"""

model_accuracies_otimization_5 = {
    'Naive Bayes': accuracy_nb,
    'Logistic Regression (Itr = 100000)': balanced_accuracy_logit,
    'k-Nearest Neighbors (k=100)': acc_knn,
    'Random Forest (estimator|states=100|42)': acc_rf,
    'CN2': acc_cn2,
    'Random Forest Grid': acc_rf_best,
    'Random Forest Seleção de features': acc_rf_selecao_alt,
    'Random Forest RFE': acc_rf_rfe,
    'Random Forest SelectKBest': acc_rf_k_best
}

plot_model_accuracy_vs_100(model_accuracies_otimization_5)